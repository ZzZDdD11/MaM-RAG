from typing import Literal, List
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from app.core.config import settings


RouteTarget = Literal["vector", "graph", "web", "generate"]

class RouteQuery(BaseModel):
    """
    è·¯ç”±å†³ç­–æ¨¡å‹ï¼šå†³å®šå°†é—®é¢˜åˆ†å‘åˆ°å“ªäº›æ•°æ®æºã€‚
    """
    datasources: List[RouteTarget] = Field(
        ...,
        description="Given a user question, choose one or more datasources to retrieve information from."
    )

class SemanticRouter:
    def __init__(self):
        # è·¯ç”±ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œå»ºè®®ç”¨å°æ¨¡å‹ (qwen2.5:1.5b) ä»¥æ±‚æé€Ÿ
        # å¦‚æœæ²¡æœ‰ 1.5bï¼Œç”¨ 7b ä¹Ÿè¡Œï¼Œå°±æ˜¯ç¨æ…¢ä¸€ç‚¹ç‚¹
        self.llm = ChatOllama(
            base_url="http://localhost:11434",
            model=settings.llm_model_name, 
            temperature=0,
        )
        # ç»‘å®šç»“æ„åŒ–è¾“å‡º
        self.structured_llm = self.llm.with_structured_output(RouteQuery)

    def route(self, question:str)-> List[str]: # type: ignore
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„è¯­ä¹‰è·¯ç”±å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°†ç”¨æˆ·çš„é—®é¢˜åˆ†å‘åˆ°æœ€åˆé€‚çš„æ•°æ®æºã€‚
        
        å¯é€‰æ•°æ®æºï¼š
        1. 'vector': é€‚ç”¨äºå…·ä½“çš„çŸ¿ç‰©å®šä¹‰ã€åŒ–å­¦æˆåˆ†ã€ç†åŒ–æ€§è´¨ã€å®éªŒæ•°æ®ã€å…·ä½“å¼€é‡‡æŠ€æœ¯ç­‰ï¼ˆæŸ¥æœ¬åœ°æ–‡æ¡£ï¼‰ã€‚
        2. 'graph': é€‚ç”¨äºæŸ¥è¯¢å®ä½“é—´çš„å…³ç³»ã€å…±ç”ŸçŸ¿ç‰©ã€æ‰€å±åˆ†ç±»ã€å±‚çº§ç»“æ„ç­‰ï¼ˆæŸ¥çŸ¥è¯†å›¾è°±ï¼‰ã€‚
        3. 'web': é€‚ç”¨äºæœ€æ–°çš„å¸‚åœºä»·æ ¼ã€è¡Œä¸šæ–°é—»ã€äº§é‡æ’åã€2024/2025å¹´çš„å®æ—¶ä¿¡æ¯ï¼ˆæŸ¥äº’è”ç½‘ï¼‰ã€‚
        4. 'generate': é€‚ç”¨äºç®€å•çš„é—®å€™ã€æ„Ÿè°¢ã€é€šç”¨å¸¸è¯†æˆ–å®Œå…¨æ— å…³çš„é—®é¢˜ï¼ˆä¸æ£€ç´¢ï¼Œç›´æ¥å›ç­”ï¼‰ã€‚
        
        æŒ‡å¯¼åŸåˆ™ï¼š
        - å¦‚æœé—®é¢˜æ¶‰åŠå…·ä½“äº‹å®ï¼Œä¼˜å…ˆé€‰ 'vector' æˆ– 'graph'ã€‚
        - å¦‚æœé—®é¢˜æ¶‰åŠæ—¶é—´æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚â€œæœ€è¿‘â€ã€â€œä»Šå¹´â€ï¼‰ï¼Œå¿…é¡»é€‰ 'web'ã€‚
        - å¦‚æœä¸ç¡®å®šï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªæºï¼ˆä¾‹å¦‚æ—¢æŸ¥ vector åˆæŸ¥ webï¼‰ã€‚
        """

        router_prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human","{question}"),
        ])

        router_chain = router_prompt | self.structured_llm 
        try:
            print(f"ğŸš¦ [Router] æ­£åœ¨åˆ†ææ„å›¾: {question}")
            result = router_chain.invoke({"question": question})
            
            # æ‰“å°å†³ç­–ç»“æœï¼Œæ–¹ä¾¿è°ƒè¯•
            print(f"ğŸ‘‰ [Router] å†³ç­–ç»“æœ: {result.datasources}") # type: ignore
            return result.datasources # type: ignore
            
        except Exception as e:
            print(f"âŒ [Router] è·¯ç”±å¤±è´¥ï¼Œé»˜è®¤å›é€€åˆ°å…¨é‡æ£€ç´¢: {e}")
            # å…œåº•ï¼šå¦‚æœå‡ºé”™ï¼Œé»˜è®¤æŸ¥æœ¬åœ°æ–‡æ¡£å’Œå›¾è°±
            return ["vector", "graph"]

# å•ä¾‹
router = SemanticRouter()