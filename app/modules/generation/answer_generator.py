import logging
from typing import List
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import ChatOllama
from app.core.config import settings

logger = logging.getLogger(__name__)

class AnswerGenerator:
    def __init__(self):
        self.llm = ChatOllama(
            base_url="http://localhost:11434",
            model=settings.llm_model_name,
            temperature=0.1
        )

    def _format_context(self, retrieval_content: List[str]) -> str:
        
        if retrieval_content is None:
            return "æ— ç›¸å…³æ£€ç´¢ç»“æœ"
        
        formatted_content = []
        for i,content in enumerate(retrieval_content):
            formatted_content.append(f"--- è¯æ®{i+1} ---\n{content}")

        return "\n\n".join(formatted_content)
    
    def generate(self, query:str, retrieval_content: List[str]) -> str:

        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡
        context = self._format_context(retrieval_content)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€ä¸¥è°¨çš„çŸ¿ç‰©åœ°è´¨å­¦ä¸“å®¶åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„ã€æ£€ç´¢ä¸Šä¸‹æ–‡ã€‘å›ç­”ç”¨æˆ·çš„ã€é—®é¢˜ã€‘ã€‚

        ### æ¥æºä¼˜å…ˆçº§è¯´æ˜ï¼š
        1. **[Graph Source] (çŸ¥è¯†å›¾è°±)**ï¼šæœ€æƒå¨ã€‚æ¶‰åŠçŸ¿ç‰©åˆ†ç±»ã€å…±ç”Ÿå…³ç³»ã€æ™¶ç³»ç­‰ç»“æ„åŒ–çŸ¥è¯†æ—¶ï¼Œä¼˜å…ˆé‡‡ä¿¡ã€‚
        2. **[Vector Source] (æœ¬åœ°æ–‡æ¡£)**ï¼šéå¸¸å¯é ã€‚æ¶‰åŠå…·ä½“æè¿°ã€æ€§è´¨å®šä¹‰ã€å®éªŒæ•°æ®æ—¶ï¼Œä¼˜å…ˆé‡‡ä¿¡ã€‚
        3. **[Web Source] (äº’è”ç½‘)**ï¼šè¡¥å……å‚è€ƒã€‚ä¸»è¦ç”¨äºå›ç­”æœ€æ–°çš„æ•°æ®ã€æ–°é—»æˆ–æœ¬åœ°çŸ¥è¯†åº“ä¸­æ²¡æœ‰çš„æ¦‚å¿µã€‚

        ### å›ç­”è¦æ±‚ï¼š
        1. **åŸºäºè¯æ®**ï¼šä¸¥æ ¼æ ¹æ®ä¸Šä¸‹æ–‡å›ç­”ï¼Œä¸è¦ç¼–é€ ã€‚å¦‚æœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥è¯´â€œæ ¹æ®ç°æœ‰çŸ¥è¯†åº“æ— æ³•å›ç­”â€ã€‚
        2. **ç»“æ„æ¸…æ™°**ï¼šä½¿ç”¨ Markdown æ ¼å¼ï¼Œåˆ†ç‚¹ä½œç­”ã€‚
        3. **æ ‡æ³¨æ¥æº**ï¼šåœ¨å…³é”®ç»“è®ºåé¢ï¼Œå°½é‡å°è¯•æ ‡æ³¨æ¥æºï¼Œä¾‹å¦‚ "(å‚è€ƒå›¾è°±)" æˆ– "(å‚è€ƒæ–‡æ¡£)"ã€‚
        4. **èåˆä¿¡æ¯**ï¼šä¸è¦æŠŠä¸‰ä¸ªæ¥æºå‰²è£‚å¼€ï¼Œè¦å°†å®ƒä»¬èåˆæˆä¸€æ®µé€šé¡ºçš„æ–‡å­—ã€‚
        5. **è¯­è¨€é£æ ¼**ï¼šå­¦æœ¯ã€å®¢è§‚ã€ç®€æ´ã€‚

        ### æ£€ç´¢ä¸Šä¸‹æ–‡ï¼š
        {context}
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system",system_prompt),
            ("human","{question}")
        ])

        chain = prompt | self.llm | StrOutputParser()
        # 5. æ‰§è¡Œ
        try:
            return chain.invoke({"context": context, "question": query})
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿç³»ç»Ÿé”™è¯¯ã€‚"
        pass
        
    def chitchat(self, query: str) -> str:
        """
        é—²èŠæ¨¡å¼ï¼šä¸ä¾èµ–æ£€ç´¢ç»“æœï¼Œç›´æ¥ç”¨ LLM è‡ªèº«çŸ¥è¯†å›ç­”
        """
        logger.info(f"ğŸ—£ï¸ [Generate] è¿›å…¥é—²èŠæ¨¡å¼: {query}")
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„çŸ¿ç‰©åœ°è´¨å­¦ä¸“å®¶åŠ©æ‰‹ã€‚
        å½“å‰ç”¨æˆ·çš„é—®é¢˜ä¸éœ€è¦æŸ¥é˜…èµ„æ–™ï¼Œè¯·ç›´æ¥ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†åº“ï¼Œä»¥è‡ªç„¶ã€æµç•…çš„è¯­æ°”è¿›è¡Œå¯¹è¯ã€‚
        å¦‚æœç”¨æˆ·æ˜¯åœ¨é—®å€™ï¼ˆå¦‚â€œä½ å¥½â€ï¼‰ï¼Œè¯·ç¤¼è²Œå›åº”å¹¶ç®€è¦ä»‹ç»è‡ªå·±ï¼ˆæˆ‘æ˜¯MineralRAGåŠ©æ‰‹ï¼‰ã€‚
        """

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{question}"),
        ])

        chain = prompt | self.llm | StrOutputParser()
        
        try:
            return chain.invoke({"question": query})
        except Exception as e:
            logger.error(f"é—²èŠç”Ÿæˆå¤±è´¥: {e}")
            return "ä½ å¥½ï¼æˆ‘æ˜¯ MineralRAG åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"
generator = AnswerGenerator()