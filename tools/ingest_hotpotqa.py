import sys
import os
from langchain_core.documents import Document

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from tools.load_hotpotqa import load_hotpot_samples
from app.core.vector import get_vector_store
from app.core.graph_extract import extract_and_store_graph

def ingest_hotpot_data(limit=10):
    # 1. åŠ è½½æ•°æ®
    samples = load_hotpot_samples(limit)
    
    vector_store = get_vector_store()
    
    print(f"ğŸš€ å¼€å§‹å°† {limit} æ¡ HotpotQA æ•°æ®çš„ä¸Šä¸‹æ–‡å…¥åº“...")
    print("âš ï¸ è­¦å‘Šï¼šè¿™å°†è°ƒç”¨ LLM è¿›è¡Œå›¾è°±æŠ½å–ï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")

    total_docs = []
    
    for i, sample in enumerate(samples):
        print(f"\n--- å¤„ç†ç¬¬ {i+1}/{limit} ä¸ªé—®é¢˜ä¸Šä¸‹æ–‡ ---")
        
        # å°†å­—ç¬¦ä¸²è½¬ä¸º Document å¯¹è±¡
        chunks = [
            Document(page_content=txt, metadata={"source": "hotpotqa", "question_id": i}) 
            for txt in sample["context_docs"]
        ]
        
        # 2. å‘é‡å…¥åº“
        print(f"ğŸ’¾ [Vector] å­˜å…¥ Milvus ({len(chunks)} chunks)...")
        vector_store.add_documents(chunks)
        
        # 3. å›¾è°±æŠ½å–ä¸å…¥åº“
        # HotpotQA çš„æ ¸å¿ƒå°±åœ¨è¿™é‡Œï¼çœ‹çœ‹ LLM èƒ½ä¸èƒ½æŠŠ Wiki é‡Œçš„å®ä½“å…³ç³»æŠ½å‡ºæ¥
        print(f"â›ï¸ [Graph] æŠ½å–å›¾è°±çŸ¥è¯†...")
        extract_and_store_graph(chunks)
        
    print("\nğŸ‰ å…¥åº“å®Œæˆï¼ç°åœ¨ä½ çš„æ•°æ®åº“é‡Œå·²ç»æœ‰äº† Wikipedia çš„çŸ¥è¯†ã€‚")

if __name__ == "__main__":
    # å…ˆè·‘ 5 ä¸ªè¯•è¯•æ°´ï¼Œåˆ«è´ªå¤šï¼Œå¦åˆ™è·‘ä¸€å¤©
    ingest_hotpot_data(limit=5)